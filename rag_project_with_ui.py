# -*- coding: utf-8 -*-
"""RAG_project_with_UI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bxIvFpcYr4kNK7lzO3rEjfFGdwHsHx6-
"""

!pip install --upgrade langchain langchain-community ctransformers
!pip install langchain
!pip install ctransformers
!pip install ctransformers[cuda]
!pip install langchain-community
!pip install -U sentence-transformers

import gradio as gr

!huggingface-cli login

from google.colab import drive
drive.mount('/content/drive')

# %%writefile app.py


import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import cosine

import os
from langchain_community.llms import CTransformers
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from ctransformers import AutoModelForCausalLM


class RAG :
      def __init__(self) :
            # Embedding model initialization
            from sentence_transformers import SentenceTransformer
            self.model_1_sentence_bert =  SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)

            # Mistral_model_initialization
            os.environ['XDG_CACHE_HOME'] = 'drive/MyDrive/LLM_data/model_mistral/cache/'
            self.model_2_mistral_LLM =  AutoModelForCausalLM.from_pretrained("TheBloke/Mistral-7B-Instruct-v0.1-GGUF", model_file="mistral-7b-instruct-v0.1.Q4_K_M.gguf", model_type="mistral", gpu_layers=50)

            # Embedding using SentenceBERT and return data_list
            data = pd.read_csv("/content/wiki_page_content_on_fraud_websites.csv")
            self.data_list = []
            N=100
            for d in list(data['page_content']):
              for batch in range(round(len(d)/N)):
                self.data_list.append(d[batch*N:batch*(N+1)])

            self.embedding = (self.model_1_sentence_bert.encode(self.data_list))

            return None



      def perform_query_embedding(self, query) :
            # query_embedding
            query_embedding = self.model_1_sentence_bert.encode(query)
            #print(query)


            return query_embedding


      def finding_most_similar_chunk(self, query_embedding):
          # finding most similar chunk and retrieving it
            max_similarity = -1
            most_similar_chunk = None
            for i in range(len(self.embedding)):
                similarity = 1 - cosine(self.embedding[i], query_embedding)
                if similarity > max_similarity:
                    max_similarity = similarity
                    most_similar_chunk = self.data_list[i]
                    # print(most_similar_chunk)
                    # print(max_similarity)

            return max_similarity, most_similar_chunk

      def generate_answer(self,query):
            query_embedding = self.perform_query_embedding(query)
            most_similar_chunk = self.finding_most_similar_chunk(query_embedding)
            self.response = self.model_2_mistral_LLM(f"[INST] You are an agent who answers questions about preferences of a person. Based on the information provided below about a personality in INFO, you need to answer question given in QUESTION . Restrict your knowledge by using only the information provided below. INFO: {most_similar_chunk}   Dont go beyond this information provided. QUESTION : {query} [/INST]")
            print(" \n ")
            print("User Question : " + str(query))
            print("Generated Answer : " + str(self.response))





# query_question_1 = "What is phishing"
# answer_1 = RAG_object.generate_answer(query_question_1)

# query_question_2 = "How can we protect ourselves from fraud?"
# answer_2 = RAG_object.generate_answer(query_question_2)

# query_question_3 = "What are the types of frauds ?"
# answer_3 = RAG_object.generate_answer(query_question_3)



RAG_object = RAG()

import gradio as gr

def generate_response_gradio(query):
    RAG_object.generate_answer(query)
    return str(RAG_object.response)

interface = gr.Interface(
    fn=generate_response_gradio,
    inputs=gr.Textbox(lines=3, placeholder="Enter your question"),
    outputs="text",
    title="UI_interface_for_RAG_based_Question_Answering_system",
    description="ask any damm questionnnn"
)


interface.launch()









